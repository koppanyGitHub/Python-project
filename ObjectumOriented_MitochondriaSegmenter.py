# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZvknCXEehWGnhGqAqdLwLlsAydUX3-x7
"""

"""Mitochondria_segmentation_with_UNet.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1glNGdnG6LOm6ByQ6R_WEo9_FJ-_N6lQA
"""

# -*- coding: utf-8 -*-
"""
Created on Fri Mar  5 16:40:21 2021
@author: Koppány
"""
import tensorflow as tf
from tensorflow import keras
from google.colab import drive

from keras.utils import normalize
import os
import cv2
from PIL import Image
import numpy as np
from matplotlib import pyplot as plt

from keras.models import Model
from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda

from sklearn.model_selection import train_test_split #splitting the pictures into training and testing sets
import random
import numpy as np

from keras.optimizers import Adam
from keras.callbacks import EarlyStopping, ModelCheckpoint

drive.mount('/content/drive')

class MithocondriaSegmenter:

  def __init__(self):
    self.TrainingImages = []
    self.TrainingMasks = []
    self.imgsize = -1
    self.model = None

    self.__X_train = None
    self.__X_test = None
    self.__y_train = None
    self.__y_test = None

  # a simple unet model, nothing special, i don't think that it is absolutely required to be able to write it, 'cause so many others already did it
  def __unet(self, img_height, img_width, img_channels):
      inputs = Input((img_height, img_width, img_channels))
      
      s = inputs

      #Contraction path
      c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)
      c1 = Dropout(0.1)(c1)
      c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)
      p1 = MaxPooling2D((2, 2))(c1)
      
      c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)
      c2 = Dropout(0.1)(c2)
      c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)
      p2 = MaxPooling2D((2, 2))(c2)
      
      c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)
      c3 = Dropout(0.2)(c3)
      c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)
      p3 = MaxPooling2D((2, 2))(c3)
      
      c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)
      c4 = Dropout(0.2)(c4)
      c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)
      p4 = MaxPooling2D(pool_size=(2, 2))(c4)
      
      c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)
      c5 = Dropout(0.3)(c5)
      c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)
      
      #Expansive path 
      u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)
      u6 = concatenate([u6, c4]) #concatenation is between certain layers of the contraction and expansion paths, it transfers data from one end to the other end
      c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)
      c6 = Dropout(0.2)(c6)
      c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)
      
      u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)
      u7 = concatenate([u7, c3])
      c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)
      c7 = Dropout(0.2)(c7)
      c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)
      
      u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)
      u8 = concatenate([u8, c2])
      c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)
      c8 = Dropout(0.1)(c8)
      c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)
      
      u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)
      u9 = concatenate([u9, c1], axis=3)
      c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)
      c9 = Dropout(0.1)(c9)
      c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)
      
      outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9) # 
      
      model = Model(inputs=[inputs], outputs=[outputs])
      model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # the adam optimizer is the best, but the loss function
      # can be many other, and all of them will alter the result in some way
      model.summary() # nothin essential, it just give us the whole unet structure back to check whether there are any mistakes 
      
      return model

  def LoadTrainingData(self, image_directory, mask_directory, imgsize=256, limit = None):
    self.imgsize = imgsize
    images = os.listdir(image_directory) # reading the pictures
    for i, image_name in enumerate(images):    # enumerate method adds a counter and returns the enumerate object, so quite usable in our case
      if (image_name.split('.')[1] == 'tif'): # all the pictures have .tif format. I split the names
        print(image_directory+image_name)
        image = cv2.imread(image_directory+image_name, 0) # image_directory is defined in the previous block
        image = Image.fromarray(image) # i turn the images into PIL objects
        image = image.resize((self.imgsize, self.imgsize)) # resizing them, because originally they are not 256x256
        self.TrainingImages.append(np.array(image)) # converting them into numpy array, and adding them to our list - the whole thing starts over until
        # the last picture isn't in the list
        if limit!=None and i>=limit:
          break

    masks = os.listdir(mask_directory) #same thing with the masks as with the pictures
    for i, image_name in enumerate(masks):
      if (image_name.split('.')[1] == 'tif'):
        image = cv2.imread(mask_directory+image_name, 0)
        image = Image.fromarray(image)
        image = image.resize((self.imgsize, self.imgsize))
        self.TrainingMasks.append(np.array(image))
        if limit!=None and i>=limit:
          break

    # normalizing images, everybody does that, it's a basic thing in neuronal networks
    self.TrainingImages = np.expand_dims(normalize(np.array(self.TrainingImages), axis=1),3)
    # not normalizing masks, just rescaling to 0 to 1, because the are already black or white
    self.TrainingMasks = np.expand_dims((np.array(self.TrainingMasks)),3) /255. # /255 is optional, but with that the list will contain only zeros or ones


  def ShowRandomSamplePair(self):
    image_number = random.randint(0, len(X_train))
    plt.figure(figsize=(12, 6))
    plt.subplot(121)
    plt.imshow(np.reshape(self.__X_train[image_number], (256, 256)), cmap='gray')
    plt.subplot(122)
    plt.imshow(np.reshape(self.__y_train[image_number], (256, 256)), cmap='gray')
    plt.show()

  def SetupUnet(self, test_size=0.1, random_state=0):
    # i have left only the 10% of the pictures for testing
    self.__X_train, self.__X_test, self.__y_train, self.__y_test = train_test_split(
        self.TrainingImages, 
        self.TrainingMasks, 
        test_size = test_size, 
        random_state = random_state)

    # checking on a few pictures whether the real images correspond with the masks
    #image_number = random.randint(0, len(X_train))
    #plt.figure(figsize=(12, 6))
    #plt.subplot(121)
    #plt.imshow(np.reshape(X_train[image_number], (256, 256)), cmap='gray')
    #plt.subplot(122)
    #plt.imshow(np.reshape(y_train[image_number], (256, 256)), cmap='gray')
    #plt.show()

    IMG_HEIGHT = self.TrainingImages.shape[1] 
    IMG_WIDTH  = self.TrainingImages.shape[2]
    IMG_CHANNELS = self.TrainingImages.shape[3]

    self.model = self.__unet(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)

  def SimpleTrainUnet(self, batch_size = 5, verbose = 1, epochs = 10):
    history = model.fit(self.__X_train, self.__y_train, # training, for 10 epochs, whit the batch size of 5. There are other options as well, i used these.
                    batch_size = batch_size, 
                    verbose=verbose, 
                    epochs=epochs, # so with these numbers the training took more, than half an our, so if we want better results, we need more epochs
                    validation_data=(self.__X_test, self.__y_test), 
                    shuffle=False)
    
  def SaveUnetWeights(self, directory):
    model.save(directory) # at the end of each epochs, the best results will be saved into this file, and with that we can predict later

  def CheckpointTrainUnetADAM(self, directory):
    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])
    model.save(directory) # at the end of each epochs, the best results will be saved into this file, and with that we can predict later
    
    checkpointer = ModelCheckpoint('model.h5', verbose=1 , mode='auto', monitor='loss',save_best_only=True)
    
    def generate_inputs(X,Y):
        for i in range(len(X)):
            X_input = X[i].reshape(1,500,500,3)
            Y_input = Y[i].reshape(1,500,500,3)
            yield (X_input,Y_input) #mivel Python generator, ezért return helyett yieldet használok
    model.fit_generator(generate_inputs(self.TrainingImages,self.TrainingMasks),epochs=40,verbose=1,callbacks=[checkpointer],
                        steps_per_epoch=5,shuffle=True)
    
  def LoadUnetWeights(self, directory):
    self.model.load_weights(directory) #Trained for 10 epochs

  def PredictResult(self, test_img):
    # every pixel under 0.2 is 0, the other is 1 -> if i write a lower number, the result will be approx. the same, which 
    # is very good, because it means, that the network learned pretty well only during 10 epochs
    # altough it is not perfect, of course, so we can use our pattern-recognizing brain to help ot out :)
    test_img_norm=test_img[:,:,0][:,:,None]
    test_img_input=np.expand_dims(test_img_norm, 0)
    return (self.model.predict(test_img_input)[0,:,:,0] > 0.2).astype(np.uint8)

#Loading the data and setting up the Unet
TrainingImageDirectory = '/content/drive/MyDrive/Egyetem/Scientific python/Project/TrainingData/images/'
TrainingMaskDirectory = '/content/drive/MyDrive/Egyetem/Scientific python/Project/TrainingData/masks/'

UnetWeights50 = '/content/drive/MyDrive/Egyetem/Scientific python/Project/NNVersions/mitochondria_test_50_epochs.hdf5'

MSegmenter = MithocondriaSegmenter()
MSegmenter.LoadTrainingData(TrainingImageDirectory, TrainingMaskDirectory, limit=5)

MSegmenter.SetupUnet()

#Instead of training using pre-trained weights for prediction
MSegmenter.LoadUnetWeights(UnetWeights50)

Prediction = MSegmenter.PredictResult(MSegmenter.TrainingImages[0])

plt.figure(figsize=(16, 8))
plt.subplot(231)
plt.title('Testing Image')
plt.imshow(MSegmenter.TrainingImages[0][:,:,0], cmap='gray')
plt.subplot(232)
plt.title('Testing Label')
plt.imshow(MSegmenter.TrainingMasks[0][:,:,0], cmap='gray')
plt.subplot(233)
plt.title('Prediction on test image')
plt.imshow(Prediction, cmap='gray')
plt.show()