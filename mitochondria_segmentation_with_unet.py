# -*- coding: utf-8 -*-
"""Mitochondria_segmentation_with_UNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1glNGdnG6LOm6ByQ6R_WEo9_FJ-_N6lQA
"""

# -*- coding: utf-8 -*-
"""
Created on Fri Mar  5 16:40:21 2021

@author: Koppány
"""
import tensorflow as tf
from tensorflow import keras
from google.colab import drive
drive.mount('/content/drive')

from keras.utils import normalize
import os
import cv2
from PIL import Image
import numpy as np
from matplotlib import pyplot as plt

from keras.models import Model
from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda

print(os.listdir("drive"))


image_directory = 'drive/My Drive/Mitochondria/images/'
mask_directory = 'drive/My Drive/Mitochondria/masks/'

SIZE = 256 # this will be the general size of the pictures; standard stuff
image_dataset = []  # many ways to handle data, i wanted to use list
mask_dataset = []  

images = os.listdir(image_directory) # reading the pictures
for i, image_name in enumerate(images):    # enumerate method adds a counter and returns the enumerate object, so quite usable in our case
    if (image_name.split('.')[1] == 'tif'): # all the pictures have .tif format. I split the names
        #print(image_directory+image_name)
        image = cv2.imread(image_directory+image_name, 0) # image_directory is defined in the previous block
        image = Image.fromarray(image) # i turn the images into PIL objects
        image = image.resize((SIZE, SIZE)) # resizing them, because originally they are not 256x256
        image_dataset.append(np.array(image)) # converting them into numpy array, and adding them to our list - the whole thing starts over until
        # the last picture isn't in the list


masks = os.listdir(mask_directory) #same thing with the masks as with the pictures
for i, image_name in enumerate(masks):
    if (image_name.split('.')[1] == 'tif'):
        image = cv2.imread(mask_directory+image_name, 0)
        image = Image.fromarray(image)
        image = image.resize((SIZE, SIZE))
        mask_dataset.append(np.array(image))


# normalizing images, everybody does that, it's a basic thing in neuronal networks
image_dataset = np.expand_dims(normalize(np.array(image_dataset), axis=1),3)
# not normalizing masks, just rescaling to 0 to 1, because the are already black or white
mask_dataset = np.expand_dims((np.array(mask_dataset)),3) /255. # /255 is optional, but with that the list will contain only zeros or ones


from sklearn.model_selection import train_test_split #splitting the pictures into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(image_dataset, mask_dataset, test_size = 0.10, random_state = 0)
# i have left only the 10% of the pictures for testing

# checking on a few pictures whether the real images correspond with the masks
import random
import numpy as np
image_number = random.randint(0, len(X_train))
plt.figure(figsize=(12, 6))
plt.subplot(121)
plt.imshow(np.reshape(X_train[image_number], (256, 256)), cmap='gray')
plt.subplot(122)
plt.imshow(np.reshape(y_train[image_number], (256, 256)), cmap='gray')
plt.show()

IMG_HEIGHT = image_dataset.shape[1] 
IMG_WIDTH  = image_dataset.shape[2]
IMG_CHANNELS = image_dataset.shape[3]
#256x256x1

# a simple unet model, nothing special, i don't think that it is absolutely required to be able to write it, 'cause so many others already did it
def unet(img_height, img_width, img_channels):
    inputs = Input((img_height, img_width, img_channels))
    
    s = inputs

    #Contraction path
    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)
    c1 = Dropout(0.1)(c1)
    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)
    p1 = MaxPooling2D((2, 2))(c1)
    
    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)
    c2 = Dropout(0.1)(c2)
    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)
    p2 = MaxPooling2D((2, 2))(c2)
     
    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)
    c3 = Dropout(0.2)(c3)
    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)
    p3 = MaxPooling2D((2, 2))(c3)
     
    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)
    c4 = Dropout(0.2)(c4)
    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)
    p4 = MaxPooling2D(pool_size=(2, 2))(c4)
     
    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)
    c5 = Dropout(0.3)(c5)
    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)
    
    #Expansive path 
    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)
    u6 = concatenate([u6, c4]) #concatenation is between certain layers of the contraction and expansion paths, it transfers data from one end to the other end
    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)
    c6 = Dropout(0.2)(c6)
    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)
     
    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)
    u7 = concatenate([u7, c3])
    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)
    c7 = Dropout(0.2)(c7)
    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)
     
    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)
    u8 = concatenate([u8, c2])
    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)
    c8 = Dropout(0.1)(c8)
    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)
     
    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)
    u9 = concatenate([u9, c1], axis=3)
    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)
    c9 = Dropout(0.1)(c9)
    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)
     
    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9) # 
     
    model = Model(inputs=[inputs], outputs=[outputs])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # the adam optimizer is the best, but the loss function
    # can be many other, and all of them will alter the result in some way
    model.summary() # nothin essential, it just give us the whole unet structure back to check whether there are any mistakes 
    
    return model

def get_model():
    return unet(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)

model = get_model()



#model.load_weights('mitochondria_50_epochs.hdf5') if i have a pretrained model, in this case for 50 epochs, i can use that

history = model.fit(X_train, y_train, # training, for 10 epochs, whit the batch size of 5. There are other options as well, i used these.
                    batch_size = 5, 
                    verbose=1, 
                    epochs=10, # so with these numbers the training took more, than half an our, so if we want better results, we need more epochs
                    validation_data=(X_test, y_test), 
                    shuffle=False)
'''
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping, ModelCheckpoint
model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])'''
model.save('mitochondria_test_10_epochs.hdf5') # at the end of each epochs, the best results will be saved into this file, and with that we can predict later
'''
checkpointer = ModelCheckpoint('model.h5', verbose=1 , mode='auto', monitor='loss',save_best_only=True)
'''
'''
def generate_inputs(X,Y):
    for i in range(len(X)):
        X_input = X[i].reshape(1,500,500,3)
        Y_input = Y[i].reshape(1,500,500,3)
        yield (X_input,Y_input) #mivel Python generator, ezért return helyett yieldet használok
model.fit_generator(generate_inputs(image_dataset,mask_dataset),epochs=40,verbose=1,callbacks=[checkpointer],
                     steps_per_epoch=5,shuffle=True)
'''

# here comes the prediction
model.load_weights('mitochondria_test_10_epochs.hdf5') #Trained for 10 epochs
#model.load_weights('mitochondria_50_epochs.hdf5')  #trained for 50 epochs, we can try to compare the results, if you are interested

test_img_number = random.randint(0, len(X_test)) # i choose a random picture for prediction
test_img = X_test[test_img_number]
ground_truth=y_test[test_img_number]
test_img_norm=test_img[:,:,0][:,:,None]
test_img_input=np.expand_dims(test_img_norm, 0)
prediction = (model.predict(test_img_input)[0,:,:,0] > 0.2).astype(np.uint8)
# every pixel under 0.2 is 0, the other is 1 -> if i write a lower number, the result will be approx. the same, which 
# is very good, because it means, that the network learned pretty well only during 10 epochs
# altough it is not perfect, of course, so we can use our pattern-recognizing brain to help ot out :)
plt.figure(figsize=(16, 8))
plt.subplot(231)
plt.title('Testing Image')
plt.imshow(test_img[:,:,0], cmap='gray')
plt.subplot(232)
plt.title('Testing Label')
plt.imshow(ground_truth[:,:,0], cmap='gray')
plt.subplot(233)
plt.title('Prediction on test image')

plt.imshow(prediction, cmap='gray')

plt.show()

#plt.imsave('input.jpg', test_img[:,:,0], cmap='gray')
#plt.imsave('data/results/output2.jpg', prediction_other, cmap='gray')